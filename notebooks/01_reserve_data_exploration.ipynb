{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Reserve Data Exploration\n",
        "Extract and examine ENTSOE reserve price data from ZIP files\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "import zipfile\n",
        "import io\n",
        "import xml.etree.ElementTree as ET\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Helper function loaded\n"
          ]
        }
      ],
      "source": [
        "# Helper function to fetch data in chunks (avoid API limits)\n",
        "def fetch_entsoe_in_chunks(base_url, start_date, end_date, chunk_days=15):\n",
        "    \"\"\"\n",
        "    Fetch ENTSOE data in chunks to avoid API limits.\n",
        "    \n",
        "    Args:\n",
        "        base_url: URL without periodStart/periodEnd parameters\n",
        "        start_date: Start date string \"YYYYMMDD0000\"\n",
        "        end_date: End date string \"YYYYMMDD0000\"\n",
        "        chunk_days: Number of days per request (default 15)\n",
        "    \n",
        "    Returns:\n",
        "        Combined response content (ZIP or XML)\n",
        "    \"\"\"\n",
        "    from datetime import datetime, timedelta\n",
        "    import time\n",
        "    \n",
        "    # Parse dates\n",
        "    start = datetime.strptime(start_date, \"%Y%m%d%H%M\")\n",
        "    end = datetime.strptime(end_date, \"%Y%m%d%H%M\")\n",
        "    \n",
        "    all_zips = []\n",
        "    current = start\n",
        "    \n",
        "    while current < end:\n",
        "        next_chunk = min(current + timedelta(days=chunk_days), end)\n",
        "        \n",
        "        period_start = current.strftime(\"%Y%m%d%H%M\")\n",
        "        period_end = next_chunk.strftime(\"%Y%m%d%H%M\")\n",
        "        \n",
        "        url = f\"{base_url}&periodStart={period_start}&periodEnd={period_end}\"\n",
        "        \n",
        "        print(f\"Fetching {period_start} to {period_end}...\")\n",
        "        \n",
        "        response = requests.get(url)\n",
        "        \n",
        "        if response.status_code == 200:\n",
        "            all_zips.append(response.content)\n",
        "            print(f\"  ✓ Got {len(response.content)} bytes\")\n",
        "        else:\n",
        "            print(f\"  ✗ Error {response.status_code}: {response.text[:200]}\")\n",
        "        \n",
        "        current = next_chunk\n",
        "        time.sleep(0.5)  # Rate limiting\n",
        "    \n",
        "    # Combine all ZIP files\n",
        "    if len(all_zips) == 1:\n",
        "        return all_zips[0]\n",
        "    \n",
        "    # Merge multiple ZIPs into one\n",
        "    combined_buf = io.BytesIO()\n",
        "    with zipfile.ZipFile(combined_buf, 'w', compression=zipfile.ZIP_DEFLATED) as zout:\n",
        "        for idx, zip_content in enumerate(all_zips):\n",
        "            try:\n",
        "                ztemp = zipfile.ZipFile(io.BytesIO(zip_content))\n",
        "                for name in ztemp.namelist():\n",
        "                    data = ztemp.read(name)\n",
        "                    out_name = f\"chunk{idx}_{name}\"\n",
        "                    zout.writestr(out_name, data)\n",
        "            except zipfile.BadZipFile:\n",
        "                # Might be plain XML, wrap it\n",
        "                zout.writestr(f\"chunk{idx}_response.xml\", zip_content)\n",
        "    \n",
        "    combined_buf.seek(0)\n",
        "    return combined_buf.read()\n",
        "\n",
        "print(\"Helper function loaded\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fetching aFRR Daily data for 4 seasonal weeks in 2024...\n",
            "\n",
            "Week 1/4: 20240108 to 20240115\n",
            "Fetching 202401080000 to 202401150000...\n",
            "  ✓ Got 6939 bytes\n",
            "  ✓ Week 1 data: 6939 bytes\n",
            "\n",
            "Week 2/4: 20240408 to 20240415\n",
            "Fetching 202404080000 to 202404150000...\n"
          ]
        },
        {
          "ename": "ConnectTimeout",
          "evalue": "HTTPSConnectionPool(host='web-api.tp.entsoe.eu', port=443): Max retries exceeded with url: /api?documentType=A81&businessType=B95&processType=A51&Type_MarketAgreement.Type=A01&controlArea_Domain=10YHU-MAVIR----U&securityToken=3da11da4-cb1c-41ea-8f2e-e80e188b9e4b&periodStart=202404080000&periodEnd=202404150000 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001FF48FEA850>, 'Connection to web-api.tp.entsoe.eu timed out. (connect timeout=None)'))",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
            "File \u001b[1;32mc:\\Users\\Baris Sever\\anaconda3\\Lib\\site-packages\\urllib3\\connection.py:198\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 198\u001b[0m     sock \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39mcreate_connection(\n\u001b[0;32m    199\u001b[0m         (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dns_host, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mport),\n\u001b[0;32m    200\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout,\n\u001b[0;32m    201\u001b[0m         source_address\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_address,\n\u001b[0;32m    202\u001b[0m         socket_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket_options,\n\u001b[0;32m    203\u001b[0m     )\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "File \u001b[1;32mc:\\Users\\Baris Sever\\anaconda3\\Lib\\site-packages\\urllib3\\util\\connection.py:85\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 85\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\Baris Sever\\anaconda3\\Lib\\site-packages\\urllib3\\util\\connection.py:73\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     72\u001b[0m     sock\u001b[38;5;241m.\u001b[39mbind(source_address)\n\u001b[1;32m---> 73\u001b[0m sock\u001b[38;5;241m.\u001b[39mconnect(sa)\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
            "\u001b[1;31mTimeoutError\u001b[0m: [WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[1;31mConnectTimeoutError\u001b[0m                       Traceback (most recent call last)",
            "File \u001b[1;32mc:\\Users\\Baris Sever\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[0;32m    788\u001b[0m     conn,\n\u001b[0;32m    789\u001b[0m     method,\n\u001b[0;32m    790\u001b[0m     url,\n\u001b[0;32m    791\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[0;32m    792\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[0;32m    793\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    794\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    795\u001b[0m     retries\u001b[38;5;241m=\u001b[39mretries,\n\u001b[0;32m    796\u001b[0m     response_conn\u001b[38;5;241m=\u001b[39mresponse_conn,\n\u001b[0;32m    797\u001b[0m     preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[0;32m    798\u001b[0m     decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[0;32m    799\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[0;32m    800\u001b[0m )\n\u001b[0;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\Baris Sever\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:488\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    487\u001b[0m         new_e \u001b[38;5;241m=\u001b[39m _wrap_proxy_error(new_e, conn\u001b[38;5;241m.\u001b[39mproxy\u001b[38;5;241m.\u001b[39mscheme)\n\u001b[1;32m--> 488\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m new_e\n\u001b[0;32m    490\u001b[0m \u001b[38;5;66;03m# conn.request() calls http.client.*.request, not the method in\u001b[39;00m\n\u001b[0;32m    491\u001b[0m \u001b[38;5;66;03m# urllib3.request. It also calls makefile (recv) on the socket.\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\Baris Sever\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:464\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 464\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_conn(conn)\n\u001b[0;32m    465\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "File \u001b[1;32mc:\\Users\\Baris Sever\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:1093\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m   1092\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_closed:\n\u001b[1;32m-> 1093\u001b[0m     conn\u001b[38;5;241m.\u001b[39mconnect()\n\u001b[0;32m   1095\u001b[0m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\Baris Sever\\anaconda3\\Lib\\site-packages\\urllib3\\connection.py:704\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    703\u001b[0m sock: socket\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m|\u001b[39m ssl\u001b[38;5;241m.\u001b[39mSSLSocket\n\u001b[1;32m--> 704\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m sock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new_conn()\n\u001b[0;32m    705\u001b[0m server_hostname: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost\n",
            "File \u001b[1;32mc:\\Users\\Baris Sever\\anaconda3\\Lib\\site-packages\\urllib3\\connection.py:207\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ConnectTimeoutError(\n\u001b[0;32m    208\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    209\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m timed out. (connect timeout=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    210\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "\u001b[1;31mConnectTimeoutError\u001b[0m: (<urllib3.connection.HTTPSConnection object at 0x000001FF48FEA850>, 'Connection to web-api.tp.entsoe.eu timed out. (connect timeout=None)')",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[1;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
            "File \u001b[1;32mc:\\Users\\Baris Sever\\anaconda3\\Lib\\site-packages\\requests\\adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[0;32m    668\u001b[0m         method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[0;32m    669\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m    670\u001b[0m         body\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mbody,\n\u001b[0;32m    671\u001b[0m         headers\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m    672\u001b[0m         redirect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    673\u001b[0m         assert_same_host\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    674\u001b[0m         preload_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    675\u001b[0m         decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    676\u001b[0m         retries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries,\n\u001b[0;32m    677\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    678\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    679\u001b[0m     )\n\u001b[0;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
            "File \u001b[1;32mc:\\Users\\Baris Sever\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:841\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    839\u001b[0m     new_e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_e)\n\u001b[1;32m--> 841\u001b[0m retries \u001b[38;5;241m=\u001b[39m retries\u001b[38;5;241m.\u001b[39mincrement(\n\u001b[0;32m    842\u001b[0m     method, url, error\u001b[38;5;241m=\u001b[39mnew_e, _pool\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, _stacktrace\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39mexc_info()[\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m    843\u001b[0m )\n\u001b[0;32m    844\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
            "File \u001b[1;32mc:\\Users\\Baris Sever\\anaconda3\\Lib\\site-packages\\urllib3\\util\\retry.py:519\u001b[0m, in \u001b[0;36mRetry.increment\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    518\u001b[0m     reason \u001b[38;5;241m=\u001b[39m error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause)\n\u001b[1;32m--> 519\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, reason) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mreason\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    521\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented Retry for (url=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, new_retry)\n",
            "\u001b[1;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='web-api.tp.entsoe.eu', port=443): Max retries exceeded with url: /api?documentType=A81&businessType=B95&processType=A51&Type_MarketAgreement.Type=A01&controlArea_Domain=10YHU-MAVIR----U&securityToken=3da11da4-cb1c-41ea-8f2e-e80e188b9e4b&periodStart=202404080000&periodEnd=202404150000 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001FF48FEA850>, 'Connection to web-api.tp.entsoe.eu timed out. (connect timeout=None)'))",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[1;31mConnectTimeout\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[3], line 30\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (start, end) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(weeks_2024, \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mWeek \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/4: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart[:\u001b[38;5;241m8\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend[:\u001b[38;5;241m8\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 30\u001b[0m     week_content \u001b[38;5;241m=\u001b[39m fetch_entsoe_in_chunks(base_url, start, end, chunk_days\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7\u001b[39m)\n\u001b[0;32m     31\u001b[0m     all_zip_contents\u001b[38;5;241m.\u001b[39mappend(week_content)\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  ✓ Week \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m data: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(week_content)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m bytes\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "Cell \u001b[1;32mIn[2], line 35\u001b[0m, in \u001b[0;36mfetch_entsoe_in_chunks\u001b[1;34m(base_url, start_date, end_date, chunk_days)\u001b[0m\n\u001b[0;32m     31\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbase_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m&periodStart=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mperiod_start\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m&periodEnd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mperiod_end\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFetching \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mperiod_start\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mperiod_end\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 35\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(url)\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[0;32m     38\u001b[0m     all_zips\u001b[38;5;241m.\u001b[39mappend(response\u001b[38;5;241m.\u001b[39mcontent)\n",
            "File \u001b[1;32mc:\\Users\\Baris Sever\\anaconda3\\Lib\\site-packages\\requests\\api.py:73\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m request(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, params\u001b[38;5;241m=\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\Baris Sever\\anaconda3\\Lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\Baris Sever\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
            "File \u001b[1;32mc:\\Users\\Baris Sever\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
            "File \u001b[1;32mc:\\Users\\Baris Sever\\anaconda3\\Lib\\site-packages\\requests\\adapters.py:688\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    685\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, ConnectTimeoutError):\n\u001b[0;32m    686\u001b[0m     \u001b[38;5;66;03m# TODO: Remove this in 3.0.0: see #2811\u001b[39;00m\n\u001b[0;32m    687\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, NewConnectionError):\n\u001b[1;32m--> 688\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ConnectTimeout(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m    690\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, ResponseError):\n\u001b[0;32m    691\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RetryError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n",
            "\u001b[1;31mConnectTimeout\u001b[0m: HTTPSConnectionPool(host='web-api.tp.entsoe.eu', port=443): Max retries exceeded with url: /api?documentType=A81&businessType=B95&processType=A51&Type_MarketAgreement.Type=A01&controlArea_Domain=10YHU-MAVIR----U&securityToken=3da11da4-cb1c-41ea-8f2e-e80e188b9e4b&periodStart=202404080000&periodEnd=202404150000 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001FF48FEA850>, 'Connection to web-api.tp.entsoe.eu timed out. (connect timeout=None)'))"
          ]
        }
      ],
      "source": [
        "# Process Type = aFRR/A51\n",
        "# Market Agreement = Daily\n",
        "# Fetch one week from each season in 2024\n",
        "API_KEY = \"3da11da4-cb1c-41ea-8f2e-e80e188b9e4b\"\n",
        "\n",
        "# Define one week from each season in 2024\n",
        "# January (winter), April (spring), June (summer), October (fall)\n",
        "weeks_2024 = [\n",
        "    (\"202401080000\", \"202401150000\"),  # January week (Jan 8-15)\n",
        "    (\"202404080000\", \"202404150000\"),  # April week (Apr 8-15)\n",
        "    (\"202406100000\", \"202406170000\"),  # June week (Jun 10-17)\n",
        "    (\"202410070000\", \"202410140000\"),  # October week (Oct 7-14)\n",
        "]\n",
        "\n",
        "base_url = (\n",
        "    \"https://web-api.tp.entsoe.eu/api?\"\n",
        "    \"documentType=A81&\"\n",
        "    \"businessType=B95&\"\n",
        "    \"processType=A51&\"\n",
        "    \"Type_MarketAgreement.Type=A01&\"\n",
        "    \"controlArea_Domain=10YHU-MAVIR----U&\"\n",
        "    f\"securityToken={API_KEY}\"\n",
        ")\n",
        "\n",
        "print(\"Fetching aFRR Daily data for 4 seasonal weeks in 2024...\")\n",
        "all_zip_contents = []\n",
        "\n",
        "for i, (start, end) in enumerate(weeks_2024, 1):\n",
        "    print(f\"\\nWeek {i}/4: {start[:8]} to {end[:8]}\")\n",
        "    week_content = fetch_entsoe_in_chunks(base_url, start, end, chunk_days=7)\n",
        "    all_zip_contents.append(week_content)\n",
        "    print(f\"  ✓ Week {i} data: {len(week_content)} bytes\")\n",
        "\n",
        "# Combine all week ZIP files into one\n",
        "print(\"\\nCombining all weeks...\")\n",
        "import zipfile\n",
        "\n",
        "combined_buf = io.BytesIO()\n",
        "with zipfile.ZipFile(combined_buf, 'w', compression=zipfile.ZIP_DEFLATED) as zout:\n",
        "    for idx, zip_content in enumerate(all_zip_contents):\n",
        "        try:\n",
        "            ztemp = zipfile.ZipFile(io.BytesIO(zip_content))\n",
        "            for name in ztemp.namelist():\n",
        "                data = ztemp.read(name)\n",
        "                out_name = f\"week{idx}_{name}\"\n",
        "                zout.writestr(out_name, data)\n",
        "        except zipfile.BadZipFile:\n",
        "            # Might be plain XML, wrap it\n",
        "            zout.writestr(f\"week{idx}_response.xml\", zip_content)\n",
        "\n",
        "combined_buf.seek(0)\n",
        "afrr_d1_content = combined_buf.read()\n",
        "\n",
        "# Create a mock response object to maintain compatibility\n",
        "class MockResponse:\n",
        "    def __init__(self, content):\n",
        "        self.content = content\n",
        "        self.status_code = 200\n",
        "\n",
        "response_afrr_d1 = MockResponse(afrr_d1_content)\n",
        "print(f\"✓ Total combined aFRR Daily data: {len(afrr_d1_content)} bytes\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "responses = [response_afrr_d1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract ZIP file\n",
        "\n",
        "import io, zipfile\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "zip_file1 = zipfile.ZipFile(io.BytesIO(response_afrr_d1.content))\n",
        "\n",
        "\n",
        "\n",
        "zips = [zip_file1]  # your ZipFile objects\n",
        "\n",
        "combined_buf = io.BytesIO()\n",
        "with zipfile.ZipFile(combined_buf, 'w', compression=zipfile.ZIP_DEFLATED) as zout:\n",
        "    for i, z in enumerate(zips):\n",
        "        for name in z.namelist():\n",
        "            data = z.read(name)\n",
        "            out_name = f\"{i}_{Path(name).name}\"  # avoid name collisions\n",
        "            zout.writestr(out_name, data)\n",
        "\n",
        "combined_buf.seek(0)\n",
        "zf = zipfile.ZipFile(combined_buf, 'r')   # merged zip\n",
        "xml_files = zf.namelist()                 # flat list of all entries\n",
        "print(f\"Merged {len(xml_files)} XML files\")\n",
        "\n",
        "xml_files_afrr_d1 = zip_file1.namelist()                 # flat list of all entries\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract XML files into a single DataFrame (store raw XML text)\n",
        "import re\n",
        "\n",
        "\n",
        "rows = []\n",
        "\n",
        "for xml_file in xml_files:\n",
        "    print(f\"\\nProcessing {xml_file}...\")\n",
        "\n",
        "    # Read XML content and decode (handle BOM safely)\n",
        "    xml_bytes = zf.read(xml_file)\n",
        "    xml_text = xml_bytes.decode('utf-8-sig', errors='replace')\n",
        "\n",
        "    # Split concatenated XML documents while preserving the declaration\n",
        "    docs = [d for d in re.split(r'(?=<\\?xml\\s)', xml_text) if d.strip()]\n",
        "    if not docs:\n",
        "        docs = [xml_text]\n",
        "\n",
        "    for i, doc in enumerate(docs):\n",
        "        # Ensure declaration at start; if missing, add a minimal one\n",
        "        cleaned = doc.lstrip()\n",
        "        if not cleaned.startswith('<?xml'):\n",
        "            cleaned = '<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n' + cleaned\n",
        "\n",
        "        name = f\"{xml_file}_part_{i}\" if len(docs) > 1 else xml_file\n",
        "        rows.append({'file': name, 'xml': cleaned})\n",
        "\n",
        "xml_docs_df = pd.DataFrame(rows)\n",
        "print(f\"Collected {len(xml_docs_df)} XML documents into xml_docs_df\")\n",
        "display(xml_docs_df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from lxml import etree as LET\n",
        "print(\"Imported LET from lxml.etree\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Parse a single balancing XML into one DataFrame (metadata + all points)\n",
        "from lxml import etree as LET\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def _lname(tag: str) -> str:\n",
        "    return tag.split('}')[-1] if '}' in tag else tag\n",
        "\n",
        "\n",
        "def parse_balancing_xml_to_df(xml_text: str) -> pd.DataFrame:\n",
        "    root = LET.fromstring(xml_text.encode('utf-8'))\n",
        "\n",
        "    # Document-level metadata (leaf nodes outside Point)\n",
        "    meta = {}\n",
        "    for el in root.iter():\n",
        "        ln = _lname(el.tag)\n",
        "        if ln == 'Point':\n",
        "            continue\n",
        "        # skip if any Point descendant\n",
        "        if el.xpath('.//*[local-name()=\"Point\"]'):\n",
        "            continue\n",
        "        if len(list(el)) == 0:\n",
        "            val = (el.text or '').strip()\n",
        "            if val:\n",
        "                key = f\"doc.{ln}\"\n",
        "                # keep first occurrence\n",
        "                if key not in meta:\n",
        "                    meta[key] = val\n",
        "    # include root attributes\n",
        "    for k, v in root.attrib.items():\n",
        "        meta[f\"doc.@{k}\"] = v\n",
        "\n",
        "    rows = []\n",
        "    # Iterate TimeSeries → Period → Point\n",
        "    for ts in root.xpath('.//*[local-name()=\"TimeSeries\"]'):\n",
        "        ts_meta = {}\n",
        "        for child in ts:\n",
        "            ln = _lname(child.tag)\n",
        "            if ln in ('Period', 'timeInterval'):\n",
        "                continue\n",
        "            # capture simple leaf text under TS (one level)\n",
        "            if len(list(child)) == 0:\n",
        "                ts_meta[f\"ts.{ln}\"] = (child.text or '').strip()\n",
        "        for per in ts.xpath('.//*[local-name()=\"Period\"]'):\n",
        "            # period context\n",
        "            start = per.xpath('.//*[local-name()=\"timeInterval\"]/*[local-name()=\"start\"]/text()')\n",
        "            end = per.xpath('.//*[local-name()=\"timeInterval\"]/*[local-name()=\"end\"]/text()')\n",
        "            res = per.xpath('.//*[local-name()=\"resolution\"]/text()')\n",
        "            per_meta = {\n",
        "                'per.start': start[0] if start else None,\n",
        "                'per.end': end[0] if end else None,\n",
        "                'per.resolution': res[0] if res else None,\n",
        "            }\n",
        "            for pt in per.xpath('.//*[local-name()=\"Point\"]'):\n",
        "                row = {}\n",
        "                # Point fields (flatten one level deep)\n",
        "                for ch in list(pt):\n",
        "                    ln = _lname(ch.tag)\n",
        "                    if len(list(ch)) == 0:\n",
        "                        row[f\"pt.{ln}\"] = (ch.text or '').strip()\n",
        "                    else:\n",
        "                        # flatten nested elements under Point one level\n",
        "                        for sub in ch:\n",
        "                            row[f\"pt.{ln}.{_lname(sub.tag)}\"] = (sub.text or '').strip()\n",
        "                # Merge: document meta + TS meta + period meta + point\n",
        "                full = {**meta, **ts_meta, **per_meta, **row}\n",
        "                rows.append(full)\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "# Source XML: prefer first entry in zf/xml_files, else fallback to example file\n",
        "if 'zf' in globals() and 'xml_files' in globals() and xml_files:\n",
        "    xml_text_src = zf.read(xml_files[0]).decode('utf-8-sig', errors='replace')\n",
        "else:\n",
        "    example_path = Path('notebooks/ActivationPriceXML.txt')\n",
        "    if not example_path.exists():\n",
        "        example_path = Path('../notebooks/ActivationPriceXML.txt')\n",
        "    xml_text_src = example_path.read_text(encoding='utf-8')\n",
        "\n",
        "points_full_df = parse_balancing_xml_to_df(xml_text_src)\n",
        "print(points_full_df.shape)\n",
        "display(points_full_df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Parse all XML documents in xml_docs_df into one DataFrame (metadata + points)\n",
        "import pandas as pd\n",
        "\n",
        "if 'xml_docs_df' not in globals() or xml_docs_df.empty:\n",
        "    raise RuntimeError('xml_docs_df is empty or not defined. Build it first.')\n",
        "\n",
        "all_rows = []\n",
        "for _, r in xml_docs_df.iterrows():\n",
        "    try:\n",
        "        dfi = parse_balancing_xml_to_df(str(r['xml']))\n",
        "        dfi['file'] = r.get('file', None)\n",
        "        all_rows.append(dfi)\n",
        "    except Exception as e:\n",
        "        print(f\"Skip {r.get('file', '?')}: {e}\")\n",
        "\n",
        "points_full_df = pd.concat(all_rows, ignore_index=True) if all_rows else pd.DataFrame()\n",
        "print(f\"points_full_df combined: shape={points_full_df.shape}, columns={len(points_full_df.columns)}\")\n",
        "display(points_full_df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "points_full_df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "unique_counts = points_full_df.nunique(dropna=True).sort_values(ascending=False).to_frame(\"n_unique\")\n",
        "print(f\"DataFrame shape: {unique_counts.shape}\")\n",
        "display(unique_counts)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "points_full_df =  points_full_df[[\n",
        "    \"doc.process.processType\",\n",
        "    \"ts.type_MarketAgreement.type\",\n",
        "    \"ts.mktPSRType.psrType\",\n",
        "    \"ts.flowDirection.direction\",\n",
        "    \"per.start\",\n",
        "    \"per.end\",\n",
        "    \"per.resolution\",\n",
        "    \"pt.position\",\n",
        "    \"pt.quantity\",\n",
        "    \"pt.procurement_Price.amount\",\n",
        "    \"ts.currency_Unit.name\",\n",
        "]]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "points_full_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# parse UTC 'Z' strings\n",
        "start_utc = pd.to_datetime(points_full_df['per.start'], utc=True, errors='coerce')\n",
        "end_utc   = pd.to_datetime(points_full_df['per.end'],   utc=True, errors='coerce')\n",
        "\n",
        "# DST-aware Hungary time (Europe/Budapest)\n",
        "points_full_df['per.start_dt'] = start_utc.dt.tz_convert('Europe/Budapest')\n",
        "points_full_df['per.end_dt']   = end_utc.dt.tz_convert('Europe/Budapest')\n",
        "\n",
        "points_full_df['per.start_dt'] = start_utc.dt.tz_convert('Etc/GMT-2')\n",
        "points_full_df['per.end_dt']   = end_utc.dt.tz_convert('Etc/GMT-2')\n",
        "\n",
        "points_full_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Start of the day-period in Europe/Budapest\n",
        "start_dt = (\n",
        "    pd.to_datetime(points_full_df['per.start'], utc=True, errors='coerce')\n",
        "      .dt.tz_convert('Europe/Budapest')\n",
        ")\n",
        "\n",
        "# Parse ISO 8601 resolution (PT15M/PT30M/PT60M/PT1H...) -> minutes\n",
        "res_ex = points_full_df['per.resolution'].str.upper().str.extract(r'PT(\\d+)([HM])')\n",
        "mins = pd.to_numeric(res_ex[0], errors='coerce')\n",
        "mins = np.where(res_ex[1].eq('H'), mins * 60, mins)\n",
        "mins = pd.to_numeric(mins, errors='coerce')\n",
        "\n",
        "# Positions are 1-based; compute offset and exact time\n",
        "pos = pd.to_numeric(points_full_df['pt.position'], errors='coerce')\n",
        "offset_min = (pos - 1) * mins\n",
        "\n",
        "points_full_df['time_dt'] = start_dt + pd.to_timedelta(offset_min, unit='m')\n",
        "points_full_df['time_end_dt'] = points_full_df['time_dt'] + pd.to_timedelta(mins, unit='m')\n",
        "\n",
        "points_full_df[['per.start','per.resolution','pt.position','time_dt','time_end_dt']].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "points_full_df =  points_full_df[[\n",
        "    'per.start_dt', \n",
        "    'per.end_dt' ,\n",
        "    'time_dt',\n",
        "    'time_end_dt',\n",
        "    'per.resolution',\n",
        "    'pt.position', \n",
        "    \"doc.process.processType\",\n",
        "    \"ts.type_MarketAgreement.type\",\n",
        "    \"ts.mktPSRType.psrType\",\n",
        "    \"ts.flowDirection.direction\",\n",
        "    \"pt.quantity\",\n",
        "    \"pt.procurement_Price.amount\",\n",
        "    \"ts.currency_Unit.name\",\n",
        "]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "points_full_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "unique_counts = points_full_df.nunique(dropna=True).sort_values(ascending=False).to_frame(\"n_unique\")\n",
        "print(f\"DataFrame shape: {unique_counts.shape}\")\n",
        "display(unique_counts)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "points_full_df_test = points_full_df[\n",
        "    (points_full_df[\"doc.process.processType\"] == \"A51\")\n",
        "    & (points_full_df[\"ts.type_MarketAgreement.type\"] == \"A01\")\n",
        "    & (points_full_df[\"ts.mktPSRType.psrType\"] == \"A03\")\n",
        "    ] \n",
        "# aFrr and BESS & daily agreement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "points_full_df_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "points_full_df_test.groupby(\"ts.flowDirection.direction\").count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "points_full_df_test.sort_values(\"time_dt\", ascending=True).head(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "unique_counts = points_full_df_test.nunique(dropna=True).sort_values(ascending=False).to_frame(\"n_unique\")\n",
        "print(f\"DataFrame shape: {unique_counts.shape}\")\n",
        "display(unique_counts)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
